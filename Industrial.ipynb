{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, settings and dataset view\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from collections import Counter\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Set pandas to show all columns when you print a dataframe\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the datasets\n",
    "df_list= [pd.read_csv('Class/binaryAllNaturalPlusNormalVsAttacks/data{}.csv'.format(i)) for i in range(1, 15)]\n",
    "# Union all the datasets into one\n",
    "df_bin = pd.concat(df_list)\n",
    "df_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about the dataset\n",
    "df_bin.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the datasets\n",
    "df_list= [pd.read_csv('Class/triple/data{}.csv'.format(i)) for i in range(1, 15)]\n",
    "# Union all the datasets into one\n",
    "df_trip = pd.concat(df_list)\n",
    "df_trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about the dataset\n",
    "df_trip.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing library\n",
    "import os\n",
    "\n",
    "# Getting all the arff files from the Class/multiclass/ directory\n",
    "# files = [arff for arff in os.listdir('.') if arff.endswith(\".arff\")]\n",
    "files = [arff for arff in os.listdir('Class/multiclass/') if arff.endswith(\".arff\")]\n",
    "files = [os.path.join('Class/multiclass/', file) for file in files]\n",
    "\n",
    "# Function for converting arff list to csv list\n",
    "def toCsv(text):\n",
    "    data = False\n",
    "    header = \"\"\n",
    "    new_content = []\n",
    "    for line in text:\n",
    "        if not data:\n",
    "            if \"@ATTRIBUTE\" in line or \"@attribute\" in line:\n",
    "                attributes = line.split()\n",
    "                if(\"@attribute\" in line):\n",
    "                    attri_case = \"@attribute\"\n",
    "                else:\n",
    "                    attri_case = \"@ATTRIBUTE\"\n",
    "                column_name = attributes[attributes.index(attri_case) + 1]\n",
    "                header = header + column_name + \",\"\n",
    "            elif \"@DATA\" in line or \"@data\" in line:\n",
    "                data = True\n",
    "                header = header[:-1]\n",
    "                header += '\\n'\n",
    "                new_content.append(header)\n",
    "        else:\n",
    "            new_content.append(line)\n",
    "    return new_content\n",
    "\n",
    "\n",
    "# Main loop for reading and writing files\n",
    "for file in files:\n",
    "    with open(file, \"r\") as inFile:\n",
    "        content = inFile.readlines()\n",
    "        name, ext = os.path.splitext(inFile.name)\n",
    "        new = toCsv(content)\n",
    "        # Change the name so everything before the first space is the name\n",
    "        name = name.split(\" \")[0]\n",
    "        with open(name + \".csv\", \"w\") as outFile:\n",
    "            outFile.writelines(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read .csv.arff (weka) and convert it to .csv \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "# Read all the files\n",
    "df_list= [pd.read_csv('Class/multiclass/data{}.csv'.format(i)) for i in range(1, 15)]\n",
    "df_mult = pd.concat(df_list)\n",
    "df_mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about the dataset\n",
    "df_mult.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = pd.concat([df_bin['marker'], df_trip['marker'], df_mult['marker']], axis=1)\n",
    "markers.columns = ['binary', 'triple', 'multiclass']\n",
    "markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a graph (sunburst) to show the distribution of the data\n",
    "px.sunburst(markers, path=['binary','triple','multiclass'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural Events\n",
    "\n",
    "| Scenario | Natural events (SLG faults) |\n",
    "| ----- | ----- |\n",
    "| 1 | Fault from 10-19% on L1 |\n",
    "| 2 | Fault from 20-79% on L1 |\n",
    "| 3 | Fault from 80-90% on L1 |\n",
    "| 4 | Fault from 10-19% on L2 |\n",
    "| 5 | Fault from 20-79% on L2 |\n",
    "| 6 | Fault from 80-90% on L2 |\n",
    "| | <br> |\n",
    "| | **Natural events (Line maintenance)** |\n",
    "| 13 | Line L1 maintenance |\n",
    "| 14 | Line L2 maintenance |\n",
    "\n",
    "#### Regular Operation\n",
    "| Scenario | No Events (Normal operation) |\n",
    "| ----- | ----- |\n",
    "| 41 | Normal Operation load changes |\n",
    "\n",
    "#### Attack Scenarios\n",
    "| Scenario | Attack Type |\n",
    "| ----- | ----- |\n",
    "| | **Data Injection** |\n",
    "| | ***Attack Sub-type (SLG fault replay)*** |\n",
    "| 7 | Fault from 10-19% on L1 with tripping command |\n",
    "| 8 | Fault from 20-79% on L1 with tripping command |\n",
    "| 9 | Fault from 80-90% on L1 with tripping command |\n",
    "| 10 | Fault from 10-19% on L2 with tripping command |\n",
    "| 11 | Fault from 20-79% on L2 with tripping command |\n",
    "| 12 | Fault from 80-90% on L2 with tripping command |\n",
    "| | <br> |\n",
    "| | **Remote Tripping Command Injection** |\n",
    "| | ***Attack Sub-type (Command injection against single relay)*** |\n",
    "| 15 | Command Injection to R1 |\n",
    "| 16 | Command Injection to R2 |\n",
    "| 17 | Command Injection to R3 |\n",
    "| 18 | Command Injection to R4 |\n",
    "| | <br> |\n",
    "| | **Attack Sub-type (Command injection against single relay)** |\n",
    "| 19 | Command Injection to R1 and R2 |\n",
    "| 20 | Command Injection to R3 and R4 |\n",
    "| | <br> |\n",
    "| | **Relay Setting Change** |\n",
    "| | ***Attack Sub-type (Disabling relay function - single relay disabled & fault)*** |\n",
    "| 21 | Fault from 10-19% on L1 with R1 disabled & fault |\n",
    "| 22 | Fault from 20-90% on L1 with R1 disabled & fault |\n",
    "| 23 | Fault from 10-49% on L1 with R2 disabled & fault |\n",
    "| 24 | Fault from 50-79% on L1 with R2 disabled & fault |\n",
    "| 25 | Fault from 80-90% on L1 with R2 disabled & fault |\n",
    "| 26 | Fault from 10-19% on L2 with R3 disabled & fault |\n",
    "| 27 | Fault from 20-49% on L2 with R3 disabled & fault |\n",
    "| 28 | Fault from 50-90% on L2 with R3 disabled & fault |\n",
    "| 29 | Fault from 10-79% on L2 with R4 disabled & fault |\n",
    "| 30 | Fault from 80-90% on L2 with R4 disabled & fault |\n",
    "| | <br> |\n",
    "| | **Attack Sub-type (Disabling relay function - two relays disabled & fault)** |\n",
    "| 35 | Fault from 10-49% on L1 with R1 and R2 disabled & fault |\n",
    "| 36 | Fault from 50-90% on L1 with R1 and R2 disabled & fault |\n",
    "| 37 | Fault from 10-49% on L1 with R3 and R4 disabled & fault |\n",
    "| 38 | Fault from 50-90% on L1 with R3 and R4 disabled & fault |\n",
    "| | <br> |\n",
    "| | **Attack Sub-type (Disabling relay function - two relay disabled & line maintenance)** |\n",
    "| 39 | L1 maintenance with R1 and R2 disabled |\n",
    "| 40 | L1 maintenance with R1 and R2 disabled |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = {1: \"Natural events (SLG faults), Fault from 10-19% on L1\",\n",
    "             2: \"Natural events (SLG faults), Fault from 20-79% on L1\",\n",
    "             3: \"Natural events (SLG faults), Fault from 80-90% on L1\",\n",
    "             4: \"Natural events (SLG faults), Fault from 10-19% on L2\",\n",
    "             5: \"Natural events (SLG faults), Fault from 20-79% on L1\",\n",
    "             6: \"Natural events (SLG faults), Fault from 80-90% on L1\",\n",
    "             7: \"Data Injection, Attack Sub-type (SLG fault replay), Fault from 10-19% on L1 with tripping command\",\n",
    "             8: \"Data Injection, Attack Sub-type (SLG fault replay), Fault from 20-79% on L1 with tripping command\",\n",
    "             9: \"Data Injection, Attack Sub-type (SLG fault replay), Fault from 80-90% on L1 with tripping command\",\n",
    "             10: \"Data Injection, Attack Sub-type (SLG fault replay), Fault from 10-19% on L2 with tripping command\",\n",
    "             11: \"Data Injection, Attack Sub-type (SLG fault replay), Fault from 20-79% on L2 with tripping command\",\n",
    "             12: \"Data Injection, Attack Sub-type (SLG fault replay), Fault from 80-90% on L2 with tripping command\",\n",
    "             13: \"Natural events (Line maintenance), Line L1 maintenance\",\n",
    "             14: \"Natural events (Line maintenance), Line L2 maintenance\",\n",
    "             15: \"Remote Tripping Command Injection, Attack Sub-type (Command injection against single relay), Command Injection to R1\",\n",
    "             16: \"Remote Tripping Command Injection, Attack Sub-type (Command injection against single relay), Command Injection to R2\",\n",
    "             17: \"Remote Tripping Command Injection, Attack Sub-type (Command injection against single relay), Command Injection to R3\",\n",
    "             18: \"Remote Tripping Command Injection, Attack Sub-type (Command injection against single relay), Command Injection to R4\",\n",
    "             19: \"Remote Tripping Command Injection, Attack Sub-type (Command injection against single relay), Command Injection to R1 and R2\",\n",
    "             20: \"Remote Tripping Command Injection, Attack Sub-type (Command injection against single relay), Command Injection to R3 and R4\",\n",
    "             21: \"Relay Setting Change, Attack Sub-type (Disabling relay function - single relay disabled & fault), Fault from 10-19% on L1 with R1 disabled & fault\",\n",
    "             22: \"Relay Setting Change, Attack Sub-type (Disabling relay function - single relay disabled & fault), Fault from 20-90% on L1 with R1 disabled & fault\",\n",
    "             23: \"Relay Setting Change, Attack Sub-type (Disabling relay function - single relay disabled & fault), Fault from 10-49% on L1 with R2 disabled & fault\",\n",
    "             24: \"Relay Setting Change, Attack Sub-type (Disabling relay function - single relay disabled & fault), Fault from 50-79% on L1 with R2 disabled & fault\",\n",
    "             25: \"Relay Setting Change, Attack Sub-type (Disabling relay function - single relay disabled & fault), Fault from 80-90% on L1 with R2 disabled & fault\",\n",
    "             26: \"Relay Setting Change, Attack Sub-type (Disabling relay function - single relay disabled & fault), Fault from 10-19% on L2 with R3 disabled & fault\",\n",
    "             27: \"Relay Setting Change, Attack Sub-type (Disabling relay function - single relay disabled & fault), Fault from 20-49% on L2 with R3 disabled & fault\",\n",
    "             28: \"Relay Setting Change, Attack Sub-type (Disabling relay function - single relay disabled & fault), Fault from 50-90% on L2 with R3 disabled & fault\",\n",
    "             29: \"Relay Setting Change, Attack Sub-type (Disabling relay function - single relay disabled & fault), Fault from 10-79% on L2 with R4 disabled & fault\",\n",
    "             30: \"Relay Setting Change, Attack Sub-type (Disabling relay function - single relay disabled & fault), Fault from 80-90% on L2 with R4 disabled & fault\",\n",
    "             35: \"Relay Setting Change, Attack Sub-type (Disabling relay function - two relays disabled & fault), Fault from 10-49% on L1 with R1 and R2 disabled & fault\",\n",
    "             36: \"Relay Setting Change, Attack Sub-type (Disabling relay function - two relays disabled & fault), Fault from 50-90% on L1 with R1 and R2 disabled & fault\",\n",
    "             37: \"Relay Setting Change, Attack Sub-type (Disabling relay function - two relays disabled & fault), Fault from 10-49% on L1 with R3 and R4 disabled & fault\",\n",
    "             38: \"Relay Setting Change, Attack Sub-type (Disabling relay function - two relays disabled & fault), Fault from 50-90% on L1 with R3 and R4 disabled & fault\",\n",
    "             39: \"Relay Setting Change, Attack Sub-type (Disabling relay function - two relay disabled & line maintenance), L1 maintenance with R1 and R2 disabled\",\n",
    "             40: \"Relay Setting Change, Attack Sub-type (Disabling relay function - two relay disabled & line maintenance), L1 maintenance with R1 and R2 disabled\",\n",
    "             41: \"No Events (Normal operation), Normal Operation load changes\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features engineering\n",
    "apparent_impedance_measurements_headers_names = ['R1-PA:Z', 'R2-PA:Z', 'R3-PA:Z', 'R4-PA:Z']\n",
    "\n",
    "voltage_phase_angles_headers_names = ['R1-PA1:VH', 'R1-PA2:VH', 'R1-PA3:VH',\n",
    "                                      'R2-PA1:VH', 'R2-PA2:VH', 'R2-PA3:VH',\n",
    "                                      'R3-PA1:VH', 'R3-PA2:VH', 'R3-PA3:VH',\n",
    "                                      'R4-PA1:VH', 'R4-PA2:VH', 'R4-PA3:VH']\n",
    "\n",
    "current_phase_angles_headers_names = ['R1-PA4:IH', 'R1-PA5:IH', 'R1-PA6:IH',\n",
    "                                      'R2-PA4:IH', 'R2-PA5:IH', 'R2-PA6:IH',\n",
    "                                      'R3-PA4:IH', 'R3-PA5:IH', 'R3-PA6:IH',\n",
    "                                      'R4-PA4:IH', 'R4-PA5:IH', 'R4-PA6:IH']\n",
    "\n",
    "voltage_phase_magnitudes_headers_names = ['R1-PM1:V', 'R1-PM2:V', 'R1-PM3:V',\n",
    "                                          'R2-PM1:V', 'R2-PM2:V', 'R2-PM3:V',\n",
    "                                          'R3-PM1:V', 'R3-PM2:V', 'R3-PM3:V',\n",
    "                                          'R4-PM1:V', 'R4-PM2:V', 'R4-PM3:V']\n",
    "\n",
    "current_phase_magnitudes_header_names = ['R1-PM4:I', 'R1-PM5:I', 'R1-PM6:I',\n",
    "                                         'R2-PM4:I', 'R2-PM5:I', 'R2-PM6:I',\n",
    "                                         'R3-PM4:I', 'R3-PM5:I', 'R3-PM6:I',\n",
    "                                         'R4-PM4:I', 'R4-PM5:I', 'R4-PM6:I']\n",
    "\n",
    "# Apparent Impedance measurements for each relay (R1-PA:Z, R2-PA:Z, R3-PA:Z, R4-PA:Z), having values in the 4.8 to 4.9 range\n",
    "for header in apparent_impedance_measurements_headers_names:\n",
    "    df_mult[header+'_in_range(4.8-4.9)'] = np.where((df_mult[header] >= 4.8) & (df_mult[header] <= 4.9), 1, 0)\n",
    "\n",
    "# Voltage Phase Angles (PA1:VH – PA3:VH) in the 3.0 range\n",
    "for header in voltage_phase_angles_headers_names:\n",
    "    df_mult[header + '_in_range(3.0)'] = np.where(abs(df_mult[header]-3.0) <= 0.5, 1, 0)\n",
    "\n",
    "# Current Phase Angles (PA4:IH – PA6:IH) in the 3.0 range\n",
    "for header in current_phase_angles_headers_names:\n",
    "    df_mult[header + '_in_range(3.0)'] = np.where(abs(df_mult[header]-3.0) <= 0.5, 1, 0)\n",
    "\n",
    "# Voltage Phase Magnitudes (PM1:V – PM3:V) in the 3.0 range\n",
    "for header in voltage_phase_magnitudes_headers_names:\n",
    "    df_mult[header + '_in_range(3.0)'] = np.where(abs(df_mult[header]-3.0) <= 0.5, 1, 0)\n",
    "\n",
    "# Current Phase Magnitudes (PM4:I – PM6:I) in the 3.0 range\n",
    "for header in current_phase_magnitudes_header_names:\n",
    "    df_mult[header + '_in_range(3.0)'] = np.where(abs(df_mult[header]-3.0) <= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all NAN columns or replace with desired string\n",
    "# This loop iterates over all of the column names which are all NaN\n",
    "for column in df_mult.columns[df_mult.isna().any()].tolist():\n",
    "    # df.drop(column, axis=1, inplace=True)\n",
    "    df_mult[column] = df_mult[column].fillna(0.0)\n",
    "\n",
    "# If you want to detect columns that may have only some NaN values use this:\n",
    "# df.loc[:, df.isna().any()].tolist()\n",
    "\n",
    "df_mult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mult.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mult = df_mult.replace([np.inf, -np.inf], np.nan)\n",
    "df_mult = df_mult.dropna()\n",
    "df_mult = df_mult.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelEncoder encodes labels with a value between 0 and n_classes-1\n",
    "le = LabelEncoder()\n",
    "# StandardScaler scales values by subtracting the mean and dividing by the standard deviation\n",
    "ss = StandardScaler()\n",
    "# QuantileTransformer transforms features using quantiles information\n",
    "qt = QuantileTransformer()\n",
    "# RobustScaler scales values by subtracting the median and dividing by the interquartile range\n",
    "rs = RobustScaler()\n",
    "# MinMaxScaler scales values between 0 and 1\n",
    "mms = MinMaxScaler()\n",
    "# LogTransformer transforms features by taking the natural logarithm\n",
    "lt = FunctionTransformer(np.log1p)\n",
    "# Preprocessing\n",
    "def vectorize_df(df):\n",
    "    df_numeric = df.select_dtypes(include=[np.number])\n",
    "    # Perform label encoder on marked column\n",
    "    df['marker'] = le.fit_transform(df['marker'])\n",
    "    for column in df_numeric.columns:\n",
    "        if column == 'marker':\n",
    "            continue\n",
    "        column_data = df_numeric[column]\n",
    "        # To avoid Input X contains infinity or a value too large for dtype('float64') error we replace them with float.max\n",
    "        column_data = column_data.replace([np.inf, -np.inf], np.finfo(np.float64).max)\n",
    "        # Check if the data is normally distributed\n",
    "        if column_data.skew() < 0.5:\n",
    "            df_numeric[column] = ss.fit_transform(column_data.values.reshape(-1,1))\n",
    "        # Check if the data has extreme outliers\n",
    "        elif column_data.quantile(0.25) < -3 or column_data.quantile(0.75) > 3:\n",
    "            df_numeric[column] = rs.fit_transform(column_data.values.reshape(-1,1))\n",
    "        # Check if the data has a Gaussian-like distribution\n",
    "        elif 0.5 < column_data.skew() < 1:\n",
    "            df_numeric[column] = lt.fit_transform(column_data.values.reshape(-1,1))\n",
    "        # Check if the data can be transformed into a Gaussian-like distribution\n",
    "        elif column_data.skew() > 1:\n",
    "            df_numeric[column] = qt.fit_transform(column_data.values.reshape(-1,1))\n",
    "        else:\n",
    "            df_numeric[column] = mms.fit_transform(column_data.values.reshape(-1,1))\n",
    "            df[df_numeric.columns] = df_numeric\n",
    "    return df\n",
    "\n",
    "df_mult = vectorize_df(df_mult)\n",
    "df_mult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose features for the model\n",
    "features_list = df_mult.columns.to_list()\n",
    "features_list.remove('marker')\n",
    "features_list.remove('index')\n",
    "features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a scatter plot of the data\n",
    "def draw_scatter_plot(df, features_list, title):\n",
    "    fig = px.scatter(df, x=features_list[0], y=features_list[1], color='marker', title=title)\n",
    "    fig.show()\n",
    "\n",
    "draw_scatter_plot(df_mult, features_list, \"Scatter plot of the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a heatmap of the data\n",
    "def draw_heatmap(df, title):\n",
    "    fig = px.imshow(df.corr(), title=title)\n",
    "    fig.show()\n",
    "\n",
    "draw_heatmap(df_mult, \"Heatmap of the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X = df_mult[features_list]\n",
    "y = np.stack(df_mult['marker'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "counter = Counter(y)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "x_after_pca_in_2D = pca.fit_transform(ss.fit_transform(df_mult[features_list].to_numpy()))\n",
    "plt.scatter(x_after_pca_in_2D[:, 0], x_after_pca_in_2D[:, 1], c=df_mult['marker'].map({p:p for p in range(0,36)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(ss.fit_transform(df_mult[features_list].to_numpy()))\n",
    "\n",
    "exp_var_cumul = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "px.area(\n",
    "    x=range(1, exp_var_cumul.shape[0] + 1),\n",
    "    y=exp_var_cumul,\n",
    "    labels={\"x\": \"# Components\", \"y\": \"Explained Variance\"},\n",
    "    range_y=(0.98, 1.02),\n",
    "    title=\"SVD Explained Variance Ratio\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Create the RFE object and compute a cross-validated score.\n",
    "# The \"accuracy\" scoring is proportional to the number of correct classifications\n",
    "recall_scorer = make_scorer(recall_score, pos_label=1, average='macro')\n",
    "rfecv = RFECV(estimator=RandomForestClassifier(n_estimators=750,criterion= \"entropy\",max_depth= 20, min_samples_split= 2, random_state=43, n_jobs=-1), step=1, cv=StratifiedKFold(2), scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "#rfecv = RFECV(estimator=LogisticRegression(), step=1, cv=StratifiedKFold(2), scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()\n",
    "\n",
    "X_train = rfecv.transform(X_train)\n",
    "X_test = rfecv.transform(X_test)\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(rfecv, open('Unification/multiclass_rfecv.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the optimal features\n",
    "optimal_features = []\n",
    "for i in range(len(rfecv.support_)):\n",
    "    if rfecv.support_[i]:\n",
    "        optimal_features.append(features_list[i])\n",
    "print(\"Optimal features: \"+ str(optimal_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid_search(model, params):\n",
    "    # Create a grid search object which is used to find the best hyperparameters for the model\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    return GridSearchCV(estimator=model, param_grid=params, n_jobs=-1, verbose=3, cv=3, scoring='accuracy', return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(model):\n",
    "    # We print our results\n",
    "    sns.set(rc={'figure.figsize': (15, 8)})\n",
    "    predictions = model.predict(X_test)\n",
    "    true_labels = y_test\n",
    "    cf_matrix = confusion_matrix(true_labels, predictions)\n",
    "    model_report = classification_report(true_labels, predictions, digits=5)\n",
    "    heatmap = sns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt='g', xticklabels=np.unique(true_labels), yticklabels=np.unique(true_labels))\n",
    "\n",
    "    # The heatmap is cool but this is the most important result\n",
    "    print(model_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier()\n",
    "rf_params = {\n",
    "    \"n_estimators\": [150, 250, 750],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": [20],\n",
    "    \"min_samples_split\": [2],\n",
    "    \"random_state\": [43],\n",
    "}\n",
    "rf_grid = create_grid_search(rf, rf_params)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "rf = rf_grid.best_estimator_\n",
    "pickle.dump(rf, open('Unification/multiclass_rfc.pkl', 'wb'))\n",
    "show(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier + AdaBoost\n",
    "rf_ada = AdaBoostClassifier(base_estimator=rf)\n",
    "rf_ada_params = {\n",
    "    'n_estimators': [50, 100, 150, 200, 250, 300, 350, 400, 450, 500],\n",
    "    'learning_rate': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "rf_ada_gcv = create_grid_search(rf_ada, rf_ada_params)\n",
    "rf_ada_gcv.fit(X_train, y_train)\n",
    "\n",
    "rf_ada = rf_ada_gcv.best_estimator_\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(rf_ada, open('Unification/multiclass_rf_ada.pkl', 'wb'))\n",
    "\n",
    "show(rf_ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K Nearest Neighbors\n",
    "knn = KNeighborsClassifier()\n",
    "knn_params = {\n",
    "    \"n_neighbors\": [3],\n",
    "    \"weights\": [\"distance\"],\n",
    "    \"algorithm\": [\"auto\"],\n",
    "    \"leaf_size\": [10],\n",
    "    \"p\": [1]\n",
    "}\n",
    "knn_grid = create_grid_search(knn, knn_params)\n",
    "knn_grid.fit(X_train, y_train)\n",
    "knn = knn_grid.best_estimator_\n",
    "pickle.dump(knn, open('Unification/multiclass_knn.pkl', 'wb'))\n",
    "show(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Classifier\n",
    "nn = MLPClassifier()\n",
    "nn_params = {\n",
    "    \"hidden_layer_sizes\": [(100, 100, 100, 100, 100)],\n",
    "    \"activation\": [\"tanh\"],\n",
    "    \"solver\": [\"adam\"],\n",
    "    \"alpha\": [0.01],\n",
    "    \"learning_rate\": [\"adaptive\"],\n",
    "}   \n",
    "nn_grid = create_grid_search(nn, nn_params)\n",
    "nn_grid.fit(X_train, y_train)\n",
    "nn = nn_grid.best_estimator_\n",
    "pickle.dump(nn, open('Unification/multiclass_nn.pkl', 'wb'))\n",
    "show(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking Classifier ( Combining all the models )\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "sc = StackingClassifier(estimators=[('rf_ada', rf_ada), ('knn', knn), ('nn', nn)], final_estimator=LogisticRegression())\n",
    "sc.fit(X_train, y_train)\n",
    "pickle.dump(sc, open('Unification/multiclass_sc.pkl', 'wb'))\n",
    "show(sc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
