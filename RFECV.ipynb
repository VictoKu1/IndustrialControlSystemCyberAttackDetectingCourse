{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, settings and dataset view\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from collections import Counter\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Set pandas to show all columns when you print a dataframe\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global settings for dataset choosing\n",
    "dataset = [\"binaryAllNaturalPlusNormalVsAttacks\", \"multiclass\", \"triple\"]\n",
    "number = [n for n in range(1, 15)]\n",
    "index = 0\n",
    "model_list = []\n",
    "rfecv_list = []\n",
    "result_list = []\n",
    "parameter=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_df(df):\n",
    "    df_numeric = df.select_dtypes(include=[np.number])\n",
    "    # Perform label encoder on marked column\n",
    "    df['marker'] = le.fit_transform(df['marker'])\n",
    "    for column in df_numeric.columns:\n",
    "        if column == 'marker':\n",
    "            continue\n",
    "        column_data = df_numeric[column]\n",
    "        # To avoid Input X contains infinity or a value too large for dtype('float64') error we replace them with float.max\n",
    "        column_data = column_data.replace([np.inf, -np.inf], np.finfo(np.float64).max)\n",
    "        # Check if the data is normally distributed\n",
    "        if column_data.skew() < 0.5:\n",
    "            df_numeric[column] = ss.fit_transform(column_data.values.reshape(-1,1))\n",
    "        # Check if the data has extreme outliers\n",
    "        elif column_data.quantile(0.25) < -3 or column_data.quantile(0.75) > 3:\n",
    "            df_numeric[column] = rs.fit_transform(column_data.values.reshape(-1,1))\n",
    "        # Check if the data has a Gaussian-like distribution\n",
    "        elif 0.5 < column_data.skew() < 1:\n",
    "            df_numeric[column] = lt.fit_transform(column_data.values.reshape(-1,1))\n",
    "        # Check if the data can be transformed into a Gaussian-like distribution\n",
    "        elif column_data.skew() > 1:\n",
    "            df_numeric[column] = qt.fit_transform(column_data.values.reshape(-1,1))\n",
    "        else:\n",
    "            df_numeric[column] = mms.fit_transform(column_data.values.reshape(-1,1))\n",
    "            df[df_numeric.columns] = df_numeric\n",
    "    return df\n",
    "\n",
    "def create_grid_search(model, params):\n",
    "    # Create a grid search object which is used to find the best hyperparameters for the model\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    return GridSearchCV(estimator=model, param_grid=params, n_jobs=-1, verbose=3, cv=3, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "def show(model, X_test, y_test):\n",
    "    # We print our results\n",
    "    sns.set(rc={'figure.figsize': (15, 8)})\n",
    "    predictions = model.predict(X_test)\n",
    "    true_labels = y_test\n",
    "    cf_matrix = confusion_matrix(true_labels, predictions)\n",
    "    model_report = classification_report(true_labels, predictions, digits=5)\n",
    "    heatmap = sns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt='g', xticklabels=np.unique(true_labels), yticklabels=np.unique(true_labels))\n",
    "\n",
    "    # The heatmap is cool but this is the most important result\n",
    "    print(model_report)\n",
    "    return model_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engeneering(df):\n",
    "    # Features engineering\n",
    "    apparent_impedance_measurements_headers_names = ['R1-PA:Z', 'R2-PA:Z', 'R3-PA:Z', 'R4-PA:Z']\n",
    "\n",
    "    voltage_phase_angles_headers_names = ['R1-PA1:VH', 'R1-PA2:VH', 'R1-PA3:VH',\n",
    "                                          'R2-PA1:VH', 'R2-PA2:VH', 'R2-PA3:VH',\n",
    "                                          'R3-PA1:VH', 'R3-PA2:VH', 'R3-PA3:VH',\n",
    "                                          'R4-PA1:VH', 'R4-PA2:VH', 'R4-PA3:VH']\n",
    "\n",
    "    current_phase_angles_headers_names = ['R1-PA4:IH', 'R1-PA5:IH', 'R1-PA6:IH',\n",
    "                                          'R2-PA4:IH', 'R2-PA5:IH', 'R2-PA6:IH',\n",
    "                                          'R3-PA4:IH', 'R3-PA5:IH', 'R3-PA6:IH',\n",
    "                                          'R4-PA4:IH', 'R4-PA5:IH', 'R4-PA6:IH']\n",
    "\n",
    "    voltage_phase_magnitudes_headers_names = ['R1-PM1:V', 'R1-PM2:V', 'R1-PM3:V',\n",
    "                                              'R2-PM1:V', 'R2-PM2:V', 'R2-PM3:V',\n",
    "                                              'R3-PM1:V', 'R3-PM2:V', 'R3-PM3:V',\n",
    "                                              'R4-PM1:V', 'R4-PM2:V', 'R4-PM3:V']\n",
    "\n",
    "    current_phase_magnitudes_header_names = ['R1-PM4:I', 'R1-PM5:I', 'R1-PM6:I',\n",
    "                                             'R2-PM4:I', 'R2-PM5:I', 'R2-PM6:I',\n",
    "                                             'R3-PM4:I', 'R3-PM5:I', 'R3-PM6:I',\n",
    "                                             'R4-PM4:I', 'R4-PM5:I', 'R4-PM6:I']\n",
    "\n",
    "    # Apparent Impedance measurements for each relay (R1-PA:Z, R2-PA:Z, R3-PA:Z, R4-PA:Z), having values in the 4.8 to 4.9 range\n",
    "    for header in apparent_impedance_measurements_headers_names:\n",
    "        df[header+'_in_range(4.8-4.9)'] = np.where((df[header] >= 4.8) & (df[header] <= 4.9), 1, 0)\n",
    "\n",
    "    # Voltage Phase Angles (PA1:VH – PA3:VH) in the 3.0 range\n",
    "    for header in voltage_phase_angles_headers_names:\n",
    "        df[header +'_in_range(3.0)'] = np.where(abs(df[header]-3.0) <= 0.5, 1, 0)\n",
    "\n",
    "    # Current Phase Angles (PA4:IH – PA6:IH) in the 3.0 range\n",
    "    for header in current_phase_angles_headers_names:\n",
    "        df[header +'_in_range(3.0)'] = np.where(abs(df[header]-3.0) <= 0.5, 1, 0)\n",
    "\n",
    "    # Voltage Phase Magnitudes (PM1:V – PM3:V) in the 3.0 range\n",
    "    for header in voltage_phase_magnitudes_headers_names:\n",
    "        df[header +'_in_range(3.0)'] = np.where(abs(df[header]-3.0) <= 0.5, 1, 0)\n",
    "\n",
    "    # Current Phase Magnitudes (PM4:I – PM6:I) in the 3.0 range\n",
    "    for header in current_phase_magnitudes_header_names:\n",
    "        df[header +'_in_range(3.0)'] = np.where(abs(df[header]-3.0) <= 0.5, 1, 0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for number_index in range(15):\n",
    "    print(\"Currently working on \"+str(dataset[index])+\"/data\"+str(number[number_index])+\".csv\")\n",
    "    relevant = \"./Class/\"+str(dataset[index])+\"/data\"+str(number[number_index])+\".csv\"\n",
    "    with open(relevant, 'rb') as file:\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "    df = feature_engeneering(df)\n",
    "    \n",
    "    for column in df.columns[df.isna().any()].tolist():\n",
    "        df[column] = df[column].fillna(0.0)\n",
    "    \n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # LabelEncoder encodes labels with a value between 0 and n_classes-1\n",
    "    le = LabelEncoder()\n",
    "    # StandardScaler scales values by subtracting the mean and dividing by the standard deviation\n",
    "    ss = StandardScaler()\n",
    "    # QuantileTransformer transforms features using quantiles information\n",
    "    qt = QuantileTransformer()\n",
    "    # RobustScaler scales values by subtracting the median and dividing by the interquartile range\n",
    "    rs = RobustScaler()\n",
    "    # MinMaxScaler scales values between 0 and 1\n",
    "    mms = MinMaxScaler()\n",
    "    # LogTransformer transforms features by taking the natural logarithm\n",
    "    lt = FunctionTransformer(np.log1p)\n",
    "    # Preprocessing\n",
    "    df = vectorize_df(df)\n",
    "\n",
    "    # Choose features for the model\n",
    "    features_list = df.columns.to_list()\n",
    "    features_list.remove('marker')\n",
    "    features_list.remove('index')\n",
    "    \n",
    "    # Train test split\n",
    "    X = df[features_list]\n",
    "    y = np.stack(df['marker'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    print(X_test.shape, y_test.shape)\n",
    "    counter = Counter(y)\n",
    "\n",
    "    # Feature selection\n",
    "\n",
    "    from sklearn.feature_selection import RFECV\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "    # Create the RFE object and compute a cross-validated score.\n",
    "    # The \"accuracy\" scoring is proportional to the number of correct classifications\n",
    "    recall_scorer = make_scorer(recall_score, pos_label=1, average='macro')\n",
    "    rfecv = RFECV(estimator=RandomForestClassifier(n_estimators=750,criterion= \"entropy\",max_depth= 20, min_samples_split= 2, random_state=43, n_jobs = -1), step=1, cv=StratifiedKFold(2), scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "    #rfecv = RFECV(estimator=LogisticRegression(), step=1, cv=StratifiedKFold(2), scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "    rfecv.fit(X_train, y_train)\n",
    "    # Save the model\n",
    "    pickle.dump(rfecv, open(\"1/m_\"+str(number[number_index])+\"rfecv.pkl\", 'wb'))\n",
    "    rfecv_list.append(rfecv)\n",
    "    X_train = rfecv.transform(X_train)\n",
    "    X_test = rfecv.transform(X_test)\n",
    "    parameter.append((X_train, X_test, y_train, y_test))\n",
    "        \n",
    "    print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "    # Plot number of features VS. cross-validation scores\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "    plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the rfecv_list\n",
    "pickle.dump(rfecv_list, open(\"1/rfecv_list.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid_search(model, params):\n",
    "    # Create a grid search object which is used to find the best hyperparameters for the model\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    return GridSearchCV(estimator=model, param_grid=params, n_jobs=-1, verbose=3, cv=10, scoring='accuracy', return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(model):\n",
    "    # We print our results\n",
    "    sns.set(rc={'figure.figsize': (15, 8)})\n",
    "    predictions = model.predict(X_test)\n",
    "    true_labels = y_test\n",
    "    cf_matrix = confusion_matrix(true_labels, predictions)\n",
    "    model_report = classification_report(true_labels, predictions, digits=5)\n",
    "    heatmap = sns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt='g', xticklabels=np.unique(true_labels), yticklabels=np.unique(true_labels))\n",
    "\n",
    "    # The heatmap is cool but this is the most important result\n",
    "    print(model_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for number_index in range(15):\n",
    "    X_train, X_test, y_train, y_test = parameter[number_index][0], parameter[number_index][1], parameter[number_index][2], parameter[number_index][3]\n",
    "    # Random Forest Classifier\n",
    "    rf = RandomForestClassifier()\n",
    "    rf_params = {\n",
    "        \"n_estimators\": [750],\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"max_depth\": [20, 35, 50],\n",
    "        \"min_samples_split\": [2,5],\n",
    "        \"random_state\": [43],\n",
    "        \"n_jobs\": [-1],\n",
    "    }\n",
    "    rf_grid = create_grid_search(rf, rf_params)\n",
    "    rf_grid.fit(X_train, y_train)\n",
    "    rf = rf_grid.best_estimator_\n",
    "    pickle.dump(rf, open('1/rfc'+str(number_index)+'.pkl', 'wb'))\n",
    "    show(rf)\n",
    "\n",
    "    # Random Forest Classifier + AdaBoost\n",
    "    rf_ada = AdaBoostClassifier(base_estimator=rf)\n",
    "    rf_ada_params = {\n",
    "        'n_estimators': [50, 100, 150, 200, 250, 300, 350, 400, 450, 500],\n",
    "        'learning_rate': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    }\n",
    "    rf_ada_grid = create_grid_search(rf_ada, rf_ada_params)\n",
    "    rf_ada_grid.fit(X_train, y_train)\n",
    "    rf_ada = rf_ada_grid.best_estimator_\n",
    "    pickle.dump(rf_ada, open('1/rf_adac'+str(number_index)+'.pkl', 'wb'))\n",
    "    show(rf_ada)\n",
    "\n",
    "    # K Nearest Neighbors\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn_params = {\n",
    "        \"n_neighbors\": [3],\n",
    "        \"weights\": [\"distance\"],\n",
    "        \"algorithm\": [\"auto\"],\n",
    "        \"leaf_size\": [10],\n",
    "        \"p\": [1],\n",
    "        \"n_jobs\": [-1],\n",
    "    }\n",
    "    knn_grid = create_grid_search(knn, knn_params)\n",
    "    knn_grid.fit(X_train, y_train)\n",
    "    knn = knn_grid.best_estimator_\n",
    "    pickle.dump(knn, open('1/knn'+str(number_index)+'.pkl', 'wb'))\n",
    "    show(knn)\n",
    "\n",
    "    # Stacking Classifier ( Combining all the models )\n",
    "    from sklearn.ensemble import StackingClassifier\n",
    "    sc = StackingClassifier(estimators=[('rf', rf), ('rf_ada', rf_ada), ('knn', knn)], final_estimator=LogisticRegression())\n",
    "    sc_params = {   \n",
    "        'n_jobs': [-1],\n",
    "    }\n",
    "    sc_grid = create_grid_search(sc, sc_params)\n",
    "    sc_grid.fit(X_train, y_train)\n",
    "    sc = sc_grid.best_estimator_\n",
    "    pickle.dump(sc, open('1/sc'+str(number_index)+'.pkl', 'wb'))\n",
    "\n",
    "    model_list.append((rf, rf_ada, knn, sc))\n",
    "    pickle.dump(model_list, open('1/model_list'+str(number_index)+'.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
